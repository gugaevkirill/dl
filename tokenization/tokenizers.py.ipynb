{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:30.249885Z",
     "start_time": "2024-10-21T19:02:30.162510Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Tokenization Task\n",
    "Write a script to tokenize a simple sentence using basic whitespace and punctuation splitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee701dcc7428995"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:30.989791Z",
     "start_time": "2024-10-21T19:02:30.255470Z"
    }
   },
   "id": "50e58ae9dc02e2d6",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello, my dog is cute.\"\n",
    "res = tokenizer.encode(sentence)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.000523Z",
     "start_time": "2024-10-21T19:02:30.994979Z"
    }
   },
   "id": "8d56a05b4a118048",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids : [101, 7592, 1010, 2026, 3899, 2003, 10140, 1012, 102]\n",
      "type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tokens : ['[CLS]', 'hello', ',', 'my', 'dog', 'is', 'cute', '.', '[SEP]']\n",
      "offsets : [(0, 0), (0, 5), (5, 6), (7, 9), (10, 13), (14, 16), (17, 21), (21, 22), (0, 0)]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing : []\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Encoding\n",
    "\n",
    "for attr in ('ids', 'type_ids', 'tokens', 'offsets', 'attention_mask', 'special_tokens_mask', 'overflowing'):\n",
    "    print(attr, ':', getattr(res, attr))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.002982Z",
     "start_time": "2024-10-21T19:02:30.998133Z"
    }
   },
   "id": "2121aed9d64250d7",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Êàë', '[UNK]', 'Â§©', '[UNK]', 'ÂÖ¨', '[UNK]', '[UNK]', '[UNK]', '„ÄÇ', '[SEP]']\n",
      "['[CLS]', 'wo', 'jin', '##tian', 'qu', 'gong', '##yuan', 'san', '##bu', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Êàë‰ªäÂ§©ÂéªÂÖ¨Âõ≠Êï£Ê≠•„ÄÇ\").tokens)\n",
    "print(tokenizer.encode(\"W«í jƒ´ntiƒÅn q√π g≈çngyu√°n s√†nb√π.\").tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.012213Z",
     "start_time": "2024-10-21T19:02:31.003323Z"
    }
   },
   "id": "ce84680aa8a61f41",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Tokenizer\n",
    "Implement a custom tokenizer from scratch that handles edge cases like contractions (e.g., \"don't\"), hyphenated words, and acronyms."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6fbab522a8b5581"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import NFC, Lowercase, BertNormalizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.013091Z",
     "start_time": "2024-10-21T19:02:31.007797Z"
    }
   },
   "id": "3795181c9fcfbb37",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I don't care about byte-pair SOTA tokenizers üòÅ.\"\n",
    "t = Tokenizer(model=BPE())\n",
    "\n",
    "res = t.encode(sentence)\n",
    "res.tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.072333Z",
     "start_time": "2024-10-21T19:02:31.014915Z"
    }
   },
   "id": "3a62f7ed69bbbf61",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't care about byte-pair SOTA tokenizers üòÅ.\n",
      "i don't care about byte-pair sota tokenizers üòÅ.\n",
      "i don't care about byte-pair sota tokenizers üòÅ.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for normalizer in (NFC(), Lowercase(), BertNormalizer()):\n",
    "    print(normalizer.normalize_str(sentence))\n",
    "\n",
    "t.normalizer = BertNormalizer()\n",
    "t.encode(sentence).tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.073069Z",
     "start_time": "2024-10-21T19:02:31.019713Z"
    }
   },
   "id": "98bd1966bbbaf0c",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', (0, 1)), (\"don't\", (2, 7)), ('care', (8, 12)), ('about', (13, 18)), ('byte-pair', (19, 28)), ('sota', (29, 33)), ('tokenizers', (34, 44)), ('üòÅ.', (45, 47))]\n",
      "[('i', (0, 1)), ('don', (2, 5)), (\"'\", (5, 6)), ('t', (6, 7)), ('care', (8, 12)), ('about', (13, 18)), ('byte', (19, 23)), ('-', (23, 24)), ('pair', (24, 28)), ('sota', (29, 33)), ('tokenizers', (34, 44)), ('üòÅ', (45, 46)), ('.', (46, 47))]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer\n",
    "\n",
    "for pre_tokenizer in (WhitespaceSplit(), BertPreTokenizer()):\n",
    "    s_norm = t.normalizer.normalize_str(sentence)\n",
    "    print(pre_tokenizer.pre_tokenize_str(s_norm))\n",
    "\n",
    "t.pre_tokenizer = BertPreTokenizer()\n",
    "t.encode(sentence).tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.073954Z",
     "start_time": "2024-10-21T19:02:31.025579Z"
    }
   },
   "id": "f1821367755d82ab",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.model = BPE()\n",
    "t.encode(sentence).tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.111349Z",
     "start_time": "2024-10-21T19:02:31.031904Z"
    }
   },
   "id": "a343f97de77dc132",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['[SEP]', '[CLS]']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "t.post_processor = BertProcessing((\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "                                  (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")))\n",
    "t.encode(sentence).tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:31.112710Z",
     "start_time": "2024-10-21T19:02:31.045145Z"
    }
   },
   "id": "5047062fc4b747ea",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('squad', split='train')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:36.375129Z",
     "start_time": "2024-10-21T19:02:31.050984Z"
    }
   },
   "id": "fb6aa84454c87ee0",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset[0]['context'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:36.378287Z",
     "start_time": "2024-10-21T19:02:36.373316Z"
    }
   },
   "id": "6764b21c723dde75",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "my_custom_dataset = (el['context'] for el in dataset)\n",
    "t.train_from_iterator(my_custom_dataset, trainer=trainer)\n",
    "t.save(\"custom_bpe_tokenizer.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.128335Z",
     "start_time": "2024-10-21T19:02:36.378311Z"
    }
   },
   "id": "3fd811ebeb8d7218",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[SEP]', 'hel', 'lo', ',', 'my', 'dog', 'is', 'cu', 'te', '[CLS]']\n",
      "['[SEP]', 'he', 'y', '[CLS]']\n",
      "['[SEP]', '‰ªä', 'Â§©', 'ÂÖ¨', '„ÄÇ', '[CLS]']\n",
      "['[SEP]', 'wo', 'j', 'int', 'ian', 'qu', 'g', 'ong', 'yuan', 'san', 'bu', '[CLS]']\n"
     ]
    }
   ],
   "source": [
    "print(t.encode(\"Hello, my dog is cute.\").tokens)\n",
    "print(t.encode('Hey I don\\'t care about byte-pair SOTA tokenizers üòÅ.').tokens)\n",
    "\n",
    "# I went to the park for a walk today.\n",
    "print(t.encode(\"Êàë‰ªäÂ§©ÂéªÂÖ¨Âõ≠Êï£Ê≠•„ÄÇ\").tokens)\n",
    "print(t.encode(\"W«í jƒ´ntiƒÅn q√π g≈çngyu√°n s√†nb√π.\").tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.132871Z",
     "start_time": "2024-10-21T19:02:42.130648Z"
    }
   },
   "id": "c793539598d7a034",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Tokenization Using NLTK\n",
    "Use the NLTK library to tokenize sentences and words from a document. Compare the results with your custom tokenizer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6435a6edd57a319"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.136072Z",
     "start_time": "2024-10-21T19:02:42.132961Z"
    }
   },
   "id": "2801e70e134fa358",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.142771Z",
     "start_time": "2024-10-21T19:02:42.134926Z"
    }
   },
   "id": "ba00fe71e6d5874a",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subword Tokenization (BPE)\n",
    "Implement Byte Pair Encoding (BPE) to tokenize a text corpus into subword units. Test this with different vocab sizes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9eca8537b5d8b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.144093Z",
     "start_time": "2024-10-21T19:02:42.136630Z"
    }
   },
   "id": "54531772c64bc97",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.145031Z",
     "start_time": "2024-10-21T19:02:42.140110Z"
    }
   },
   "id": "c1da91c90572f506",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-trained Tokenizer (Hugging Face)\n",
    "Load a pre-trained tokenizer from the Hugging Face library (e.g., BERT or GPT) and tokenize text inputs. Explore different encoding options (padding, truncation)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14c8b97b35562a5b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "t = tiktoken.get_encoding(\"cl100k_base\")\n",
    "t1 = tiktoken.encoding_for_model(\"gpt-4o\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.180923Z",
     "start_time": "2024-10-21T19:02:42.145089Z"
    }
   },
   "id": "ae9ab56fbb49a482",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 1541, 956, 2512, 922, 5027, 2320, 1334, 328, 37644, 4037, 12509, 27623, 223, 13]\n",
      "[40, 4128, 2631, 1078, 9239, 3161, 1517, 336, 61390, 6602, 24223, 22861, 223, 13]\n"
     ]
    }
   ],
   "source": [
    "print(t.encode(sentence))\n",
    "print(t1.encode(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.182188Z",
     "start_time": "2024-10-21T19:02:42.152285Z"
    }
   },
   "id": "76116ad64711a290",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't care about byte-pair SOTA tokenizers üòÅ.\n",
      "\"#$%&indowlementpectash[i use WebDesBCancial\n"
     ]
    }
   ],
   "source": [
    "# generate text based on tokens\n",
    "print(t.decode(t.encode(sentence)))\n",
    "print(t.decode([1, 2, 3, 4, 5, 1000, 1001, 1002, 1003, 1004, 1005, 5000, 5001, 5002, 5003]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.182884Z",
     "start_time": "2024-10-21T19:02:42.165719Z"
    }
   },
   "id": "2384d254503e22cc",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37046, 37271, 36827, 86436, 35417, 9921, 255, 8067, 96, 65782, 1811]\n",
      "Êàë‰ªäÂ§©ÂéªÂÖ¨Âõ≠Êï£Ê≠•„ÄÇ\n",
      "[54, 131, 240, 503, 61711, 406, 72, 31757, 77, 2874, 15273, 342, 56761, 983, 41101, 11644, 274, 6496, 18571, 15273, 13]\n",
      "W«í jƒ´ntiƒÅn q√π g≈çngyu√°n s√†nb√π.\n"
     ]
    }
   ],
   "source": [
    "for s in (\"Êàë‰ªäÂ§©ÂéªÂÖ¨Âõ≠Êï£Ê≠•„ÄÇ\", \"W«í jƒ´ntiƒÅn q√π g≈çngyu√°n s√†nb√π.\"):\n",
    "    print(t.encode(s))\n",
    "    print(t.decode(t.encode(s)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.213456Z",
     "start_time": "2024-10-21T19:02:42.170651Z"
    }
   },
   "id": "a6f109c6f50105ab",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SentencePiece Tokenization\n",
    "Use the SentencePiece library to build a subword tokenizer and tokenize a text file. Compare the performance and output with BPE."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1085827eb6be90bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:02:42.214819Z",
     "start_time": "2024-10-21T19:02:42.179533Z"
    }
   },
   "id": "dae5c03d300626f6",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: custom_sp_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: foo\n",
      "  user_defined_symbols: bar\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 87599 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: foo\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: bar\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=66148702\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=122\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 87599 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=59085213\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 242796 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 87599\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 184137\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 184137 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=119316 obj=11.8545 num_tokens=401673 num_tokens/piece=3.36646\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=92160 obj=9.43742 num_tokens=401130 num_tokens/piece=4.35254\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=69107 obj=9.40896 num_tokens=414020 num_tokens/piece=5.991\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=69089 obj=9.39443 num_tokens=414068 num_tokens/piece=5.99326\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=51815 obj=9.46822 num_tokens=441771 num_tokens/piece=8.52593\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=51810 obj=9.45548 num_tokens=441728 num_tokens/piece=8.52592\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38856 obj=9.57342 num_tokens=474569 num_tokens/piece=12.2135\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=38856 obj=9.54585 num_tokens=474584 num_tokens/piece=12.2139\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=29142 obj=9.70857 num_tokens=510178 num_tokens/piece=17.5066\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29142 obj=9.67766 num_tokens=510178 num_tokens/piece=17.5066\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21856 obj=9.87751 num_tokens=547935 num_tokens/piece=25.0702\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21856 obj=9.84145 num_tokens=547854 num_tokens/piece=25.0665\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16392 obj=10.0821 num_tokens=585132 num_tokens/piece=35.6962\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16392 obj=10.0377 num_tokens=585220 num_tokens/piece=35.7016\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12294 obj=10.3212 num_tokens=622560 num_tokens/piece=50.6393\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12294 obj=10.2699 num_tokens=622636 num_tokens/piece=50.6455\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9220 obj=10.6037 num_tokens=659847 num_tokens/piece=71.5669\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9220 obj=10.5412 num_tokens=659908 num_tokens/piece=71.5735\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6915 obj=10.9219 num_tokens=696639 num_tokens/piece=100.743\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6915 obj=10.8501 num_tokens=696651 num_tokens/piece=100.745\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5186 obj=11.2855 num_tokens=732629 num_tokens/piece=141.271\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5186 obj=11.2035 num_tokens=732669 num_tokens/piece=141.278\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3889 obj=11.6843 num_tokens=769624 num_tokens/piece=197.898\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3889 obj=11.5912 num_tokens=769641 num_tokens/piece=197.902\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2916 obj=12.136 num_tokens=805084 num_tokens/piece=276.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2916 obj=12.0347 num_tokens=805163 num_tokens/piece=276.119\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2187 obj=12.6317 num_tokens=843934 num_tokens/piece=385.887\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2187 obj=12.5131 num_tokens=843996 num_tokens/piece=385.915\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1640 obj=13.1214 num_tokens=883034 num_tokens/piece=538.435\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1640 obj=13.0015 num_tokens=882978 num_tokens/piece=538.401\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1230 obj=13.645 num_tokens=922386 num_tokens/piece=749.907\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1230 obj=13.5153 num_tokens=922465 num_tokens/piece=749.972\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=13.7617 num_tokens=938616 num_tokens/piece=853.287\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=13.7087 num_tokens=938854 num_tokens/piece=853.504\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: custom_sp_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: custom_sp_model.vocab\n"
     ]
    }
   ],
   "source": [
    "my_custom_dataset = (el['context'] for el in dataset)\n",
    "spm.SentencePieceTrainer.train(sentence_iterator=my_custom_dataset, model_prefix='custom_sp_model', vocab_size=1000, user_defined_symbols=['foo', 'bar'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.656428Z",
     "start_time": "2024-10-21T19:02:42.188572Z"
    }
   },
   "id": "37e7145b9846931c",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 't', 'est', '‚ñÅ', 's', 'ent', 'ence', '.']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='custom_sp_model.model')\n",
    "print(sp.encode(\"This is a test sentence.\", out_type=str))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.661089Z",
     "start_time": "2024-10-21T19:03:16.659475Z"
    }
   },
   "id": "b84478824fe00d3a",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅ', 'Êàë‰ªäÂ§©ÂéªÂÖ¨Âõ≠Êï£Ê≠•„ÄÇ']\n",
      "['‚ñÅW', '«í', '‚ñÅ', 'j', 'ƒ´', 'n', 'ti', 'ƒÅ', 'n', '‚ñÅ', 'q', '√π', '‚ñÅg', '≈ç', 'n', 'g', 'y', 'u', '√°', 'n', '‚ñÅ', 's', '√†', 'n', 'b', '√π', '.']\n"
     ]
    }
   ],
   "source": [
    "# I went to the park for a walk today.\n",
    "print(sp.encode(\"Êàë‰ªäÂ§©ÂéªÂÖ¨Âõ≠Êï£Ê≠•„ÄÇ\", out_type=str))\n",
    "print(sp.encode(\"W«í jƒ´ntiƒÅn q√π g≈çngyu√°n s√†nb√π.\", out_type=str))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.663920Z",
     "start_time": "2024-10-21T19:03:16.662142Z"
    }
   },
   "id": "2215d8a232c10f90",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fast Tokenization (Tokenizers Library)\n",
    "Use the Hugging Face tokenizers library to tokenize text at high speed. Experiment with different tokenization algorithms (WordPiece, BPE, etc.)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f88805a70a359b33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.665341Z",
     "start_time": "2024-10-21T19:03:16.664136Z"
    }
   },
   "id": "670bea8ecf336126",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.674001Z",
     "start_time": "2024-10-21T19:03:16.669432Z"
    }
   },
   "id": "d8b3369567c40381",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoding and Decoding\n",
    "Encode a piece of text using a tokenizer and then decode it back to the original form. Verify that the decoding process produces the same text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30119d8e5223ea0a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.676631Z",
     "start_time": "2024-10-21T19:03:16.671066Z"
    }
   },
   "id": "f4af327ac330844c",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.678378Z",
     "start_time": "2024-10-21T19:03:16.673862Z"
    }
   },
   "id": "3f366510a4aa77ee",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization on a Multilingual Dataset\n",
    "Tokenize text in multiple languages (e.g., English, French, Chinese). Analyze the differences in how the tokenizer handles different languages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "218d00074819a1d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.679780Z",
     "start_time": "2024-10-21T19:03:16.675580Z"
    }
   },
   "id": "accbc97eb39def53",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.684707Z",
     "start_time": "2024-10-21T19:03:16.677650Z"
    }
   },
   "id": "5ef47efabf658019",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build a Tokenizer Pipeline\n",
    "Chain multiple steps of a tokenizer pipeline (e.g., text cleaning, tokenization, padding, and special token addition) using a framework like Hugging Face's transformers. Test it on a text classification task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd25809d4c3a5af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T19:03:16.739397Z",
     "start_time": "2024-10-21T19:03:16.680119Z"
    }
   },
   "id": "87915656336f5a78",
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
